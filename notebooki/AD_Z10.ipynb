{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2a5066",
   "metadata": {},
   "source": [
    "# Laboratorium nr 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importy niezbdnych paczek\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74fd154",
   "metadata": {},
   "source": [
    "# Ensemble i Soft Voting\n",
    "\n",
    "## Czym s metody ensemble?\n",
    "\n",
    "Metody ensemble (ang. ensemble methods) to techniki w uczeniu maszynowym, kt贸re cz przewidywania wielu modeli (tzw. modeli bazowych) w celu uzyskania lepszej dokadnoci i stabilnoci predykcji ni偶 pojedynczy model. Ide jest wykorzystanie r贸偶norodnoci modeli, aby zredukowa bdy wynikajce z nadmiernego dopasowania (overfitting), wariancji lub obci偶enia (bias).\n",
    "\n",
    "### G贸wne typy metod ensemble:\n",
    "1. **Bagging** (Bootstrap Aggregating): Modele trenuje si na r贸偶nych podzbiorach danych (losowo wybieranych z powt贸rzeniami), a wyniki s agregowane, np. przez redni (dla regresji) lub gosowanie wikszociowe (dla klasyfikacji). Przykad: Random Forest.\n",
    "2. **Boosting**: Modele trenuje si sekwencyjnie, gdzie ka偶dy kolejny model skupia si na poprawianiu bd贸w poprzednich. Przykad: Gradient Boosting, AdaBoost.\n",
    "3. **Stacking**: Wyniki wielu modeli s czone za pomoc meta-modelu, kt贸ry uczy si, jak najlepiej wa偶y przewidywania modeli bazowych.\n",
    "4. **Voting**: Przewidywania wielu modeli s czone przez gosowanie (dla klasyfikacji) lub redni (dla regresji). Wyr贸偶niamy **hard voting** i **soft voting**.\n",
    "\n",
    "---\n",
    "\n",
    "## Soft Voting - Szczeg贸y i Matematyka\n",
    "\n",
    "### Czym jest soft voting?\n",
    "\n",
    "Soft voting to technika ensemble dla problem贸w klasyfikacji, w kt贸rej przewidywania modeli bazowych s czone na podstawie **prawdopodobiestw przynale偶noci do klas**. Ka偶dy model bazowy zwraca prawdopodobiestwa dla ka偶dej klasy, a kocowa predykcja jest obliczana jako rednia wa偶ona tych prawdopodobiestw, po czym wybierana jest klasa o najwy偶szym rednim prawdopodobiestwie.\n",
    "\n",
    "W przeciwiestwie do **hard voting**, gdzie ka偶dy model oddaje jeden \"gos\" na klas (wynik jest liczony jako wikszo gos贸w), soft voting uwzgldnia pewno modelu wyra偶on w prawdopodobiestwach, co zazwyczaj prowadzi do lepszych wynik贸w.\n",
    "\n",
    "### Formalny opis matematyczny\n",
    "\n",
    "Za贸偶my, 偶e mamy:\n",
    "- $ M $ modeli bazowych (np. drzewa decyzyjne, SVM, sieci neuronowe).\n",
    "- $ K $ klas, kt贸re przewidujemy (np. $ K = 3 $ dla klas: \"kot\", \"pies\", \"ptak\").\n",
    "- Dla ka偶dego modelu $ m $ (gdzie $ m = 1, 2, \\dots, M $) i dla ka偶dej pr贸bki $ x $, model zwraca wektor prawdopodobiestw przynale偶noci do klas:\n",
    "  $$\n",
    "  P_m(x) = [p_{m,1}(x), p_{m,2}(x), \\dots, p_{m,K}(x)],\n",
    "  $$\n",
    "  gdzie $ p_{m,k}(x) $ to prawdopodobiestwo, 偶e pr贸bka $ x $ nale偶y do klasy $ k $ wedug modelu $ m $, oraz $ \\sum_{k=1}^K p_{m,k}(x) = 1 $.\n",
    "\n",
    "#### Krok 1: Obliczenie rednich prawdopodobiestw\n",
    "Dla ka偶dej klasy $ k $, obliczamy rednie prawdopodobiestwo na podstawie wszystkich modeli:\n",
    "$$\n",
    "\\bar{p}_k(x) = \\frac{1}{M} \\sum_{m=1}^M p_{m,k}(x).\n",
    "$$\n",
    "Jeli modele maj r贸偶ne wagi $ w_m $ (gdzie $ \\sum_{m=1}^M w_m = 1 $), to:\n",
    "$$\n",
    "\\bar{p}_k(x) = \\sum_{m=1}^M w_m \\cdot p_{m,k}(x).\n",
    "$$\n",
    "\n",
    "#### Krok 2: Wyb贸r klasy\n",
    "Kocowa predykcja to klasa $ k^* $, kt贸ra ma najwy偶sze rednie prawdopodobiestwo:\n",
    "$$\n",
    "k^* = \\arg\\max_{k \\in \\{1, 2, \\dots, K\\}} \\bar{p}_k(x).\n",
    "$$\n",
    "\n",
    "### Przykad\n",
    "Za贸偶my, 偶e mamy 3 modele ($ M = 3 $) i 2 klasy ($ K = 2 $, np. \"pozytywna\" i \"negatywna\"). Dla pr贸bki $ x $, modele zwracaj nastpujce prawdopodobiestwa:\n",
    "\n",
    "- Model 1: $ P_1(x) = [0.9, 0.1] $ (90% na klas pozytywn, 10% na negatywn).\n",
    "- Model 2: $ P_2(x) = [0.7, 0.3] $.\n",
    "- Model 3: $ P_3(x) = [0.6, 0.4] $.\n",
    "\n",
    "Obliczamy rednie prawdopodobiestwa dla ka偶dej klasy:\n",
    "$$\n",
    "\\bar{p}_1(x) = \\frac{0.9 + 0.7 + 0.6}{3} = 0.733,\n",
    "$$\n",
    "$$\n",
    "\\bar{p}_2(x) = \\frac{0.1 + 0.3 + 0.4}{3} = 0.267.\n",
    "$$\n",
    "Klasa o najwy偶szym prawdopodobiestwie to klasa 1 (pozytywna), wic $ k^* = 1 $.\n",
    "\n",
    "---\n",
    "\n",
    "## Zalety i wady soft voting\n",
    "\n",
    "### Zalety:\n",
    "- Uwzgldnia pewno modeli, co prowadzi do bardziej wywa偶onych decyzji.\n",
    "- Czsto daje lepsze wyniki ni偶 hard voting, szczeg贸lnie gdy modele s dobrze skalibrowane.\n",
    "- Elastyczno w stosowaniu wag dla r贸偶nych modeli.\n",
    "\n",
    "### Wady:\n",
    "- Wymaga, aby modele zwracay dobrze skalibrowane prawdopodobiestwa.\n",
    "- Wiksza zo偶ono obliczeniowa w por贸wnaniu do hard voting.\n",
    "- Mo偶e by mniej skuteczne, jeli modele s bardzo r贸偶ne pod wzgldem jakoci predykcji.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4ad00",
   "metadata": {},
   "source": [
    "### Zadanie nr 1 (5 pkt)\n",
    "Zaimplementuj klasyfikator ensemble z soft voting w Pythonie. Por贸wnaj wyniki z wbudowanym modelem `VotingClassifier`.\n",
    "\n",
    "1. Wygeneruj syntetyczny zbi贸r danych binarnej klasyfikacji (np. za pomoc `make_classification` z scikit-learn).\n",
    "2. Zbuduj trzy r贸偶ne modele bazowe (np. Logistic Regression, Decision Tree, SVM).\n",
    "3. Zaimplementuj soft voting rcznie, obliczajc rednie prawdopodobiestwa dla ka偶dej klasy.\n",
    "4. Por贸wnaj wyniki swojej implementacji z modelem `VotingClassifier` z scikit-learn.\n",
    "5. Oce dokadno (accuracy) obu podej na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52bb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3423f331",
   "metadata": {},
   "source": [
    "# Lasy Losowe\n",
    "\n",
    "## Czym s lasy losowe?\n",
    "\n",
    "Lasy losowe (ang. Random Forests) to metoda ensemble oparta na technice **bagging** (Bootstrap Aggregating), kt贸ra czy wiele drzew decyzyjnych w celu poprawy dokadnoci i stabilnoci predykcji. Zostaa zaproponowana przez Leo Breimana w 2001 roku i jest szeroko stosowana zar贸wno w zadaniach klasyfikacji, jak i regresji.\n",
    "\n",
    "### Kluczowe cechy las贸w losowych:\n",
    "1. **Wiele drzew decyzyjnych**: Las losowy skada si z $ T $ drzew decyzyjnych, gdzie ka偶de drzewo jest trenowane na losowym podzbiorze danych.\n",
    "2. **Bootstrap**: Ka偶de drzewo jest trenowane na losowym podzbiorze danych treningowych, wybranym z powt贸rzeniami (tzw. bootstrap sample). Zazwyczaj wielko podzbioru jest r贸wna wielkoci oryginalnego zbioru danych.\n",
    "3. **Losowy wyb贸r cech**: W ka偶dym w藕le drzewa decyzyjnego wybierana jest losowa podgrupa cech (atrybut贸w), spor贸d kt贸rych wybierana jest najlepsza cecha do podziau. To zmniejsza korelacj midzy drzewami i zwiksza r贸偶norodno.\n",
    "4. **Agregacja wynik贸w**:\n",
    "   - Dla **klasyfikacji**: Wyniki drzew s czone przez gosowanie wikszociowe (ang. majority voting).\n",
    "   - Dla **regresji**: Wyniki drzew s uredniane.\n",
    "\n",
    "### Formalny opis matematyczny\n",
    "\n",
    "Za贸偶my, 偶e mamy zbi贸r danych treningowych $ D = \\{(x_1, y_1), (x_2, y_2), \\dots, (x_N, y_N)\\} $, gdzie $ x_i $ to wektor cech, a $ y_i $ to etykieta (dla klasyfikacji) lub warto ciga (dla regresji).\n",
    "\n",
    "#### Krok 1: Tworzenie podzbior贸w danych\n",
    "Dla ka偶dego drzewa $ t = 1, 2, \\dots, T $:\n",
    "- Losujemy z powt贸rzeniami podzbi贸r danych $ D_t $ o rozmiarze $ N $ (bootstrap sample).\n",
    "- Losowy podzbi贸r cech (np. $ \\sqrt{p} $ lub $ p/3 $, gdzie $ p $ to liczba wszystkich cech) jest wybierany w ka偶dym w藕le drzewa do okrelenia najlepszego podziau.\n",
    "\n",
    "#### Krok 2: Budowa drzew\n",
    "Ka偶de drzewo $ h_t(x) $ jest trenowane na zbiorze $ D_t $. Drzewo podejmuje decyzje na podstawie regu podziau opartych na wybranych cechach. W przeciwiestwie do pojedynczego drzewa decyzyjnego, las losowy ogranicza overfitting dziki r贸偶norodnoci drzew.\n",
    "\n",
    "#### Krok 3: Agregacja predykcji\n",
    "- Dla **klasyfikacji**:\n",
    "  Predykcja lasu losowego to klasa, kt贸ra otrzymaa najwicej gos贸w od drzew:\n",
    "  $$\n",
    "  \\hat{y}(x) = \\text{mode} \\{ h_1(x), h_2(x), \\dots, h_T(x) \\},\n",
    "  $$\n",
    "  gdzie $ \\text{mode} $ oznacza najczciej wystpujc klas.\n",
    "- Dla **regresji**:\n",
    "  Predykcja to rednia predykcji wszystkich drzew:\n",
    "  $$\n",
    "  \\hat{y}(x) = \\frac{1}{T} \\sum_{t=1}^T h_t(x).\n",
    "  $$\n",
    "\n",
    "#### Krok 4: Wa偶no cech (Feature Importance)\n",
    "Lasy losowe mog ocenia wa偶no cech na podstawie tego, jak bardzo ka偶da cecha przyczynia si do zmniejszenia nieczystoci (np. entropii lub wsp贸czynnika Gini) w wzach drzew. Wa偶no cechy $ j $ mo偶na zapisa jako:\n",
    "$$\n",
    "\\text{Importance}(j) = \\frac{1}{T} \\sum_{t=1}^T \\sum_{\\text{wzy } v \\text{ w drzewie } t} \\Delta i(v, j),\n",
    "$$\n",
    "gdzie $ \\Delta i(v, j) $ to zmniejszenie nieczystoci w w藕le $ v $ dziki cesze $ j $.\n",
    "\n",
    "### Zalety las贸w losowych\n",
    "- Odporno na overfitting dziki agregacji wielu drzew.\n",
    "- Dobra wydajno w zadaniach z du偶 liczb cech i szumem.\n",
    "- Mo偶liwo oceny wa偶noci cech.\n",
    "- Prosta implementacja i r贸wnolege trenowanie drzew.\n",
    "\n",
    "### Wady\n",
    "- Wysoka zo偶ono obliczeniowa przy du偶ej liczbie drzew i danych.\n",
    "- Mniejsza interpretowalno w por贸wnaniu do pojedynczego drzewa decyzyjnego.\n",
    "- Mo偶e by mniej skuteczny w zadaniach wymagajcych modelowania zo偶onych zale偶noci (np. w por贸wnaniu do gradient boosting).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0012146a",
   "metadata": {},
   "source": [
    "### Zadanie nr 2 (3 punkty)\n",
    "Zbuduj model las贸w losowych w Pythonie, u偶ywajc scikit-learn, i oce jego dokadno na syntetycznym zbiorze danych klasyfikacji binarnej.\n",
    "\n",
    "1. Wygeneruj zbi贸r danych binarnej klasyfikacji (u偶yj `make_classification`).\n",
    "2. Podziel dane na zbi贸r treningowy i testowy.\n",
    "3. Zbuduj model las贸w losowych z 50 drzewami.\n",
    "4. Oce dokadno modelu na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11753689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91011b97",
   "metadata": {},
   "source": [
    "#  Algorytm Gradient Boosting dla Klasyfikacji (Binary)\n",
    "\n",
    "##  Dane wejciowe\n",
    "\n",
    "- Zbi贸r uczcy: \\( (x_1, y_1), \\dots, (x_n, y_n) \\), gdzie \\( y_i \\in \\{0, 1\\} \\)\n",
    "- Liczba iteracji (drzew): \\( T \\)\n",
    "- Learning rate: \\( \\eta \\in (0, 1] \\)\n",
    "- Funkcja straty: log-loss\n",
    "\n",
    "---\n",
    "\n",
    "## 锔 Algorytm\n",
    "\n",
    "### 1. Inicjalizacja\n",
    "\n",
    "Oblicz pocztkow warto predykcji jako logit redniego udziau klasy pozytywnej:\n",
    "\n",
    "\\[\n",
    "\\hat{y}_0 = \\log \\left( \\frac{p}{1 - p} \\right), \\quad \\text{gdzie } p = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Dla ka偶dej iteracji \\( t = 1 \\dots T \\):\n",
    "\n",
    "#### a) Oblicz prawdopodobiestwa\n",
    "\n",
    "\\[\n",
    "p_i^{(t-1)} = \\frac{1}{1 + e^{-\\hat{y}_{t-1}(x_i)}}\n",
    "\\]\n",
    "\n",
    "#### b) Oblicz pseudoreszty (gradient log-loss)\n",
    "\n",
    "\\[\n",
    "r_i^{(t)} = y_i - p_i^{(t-1)}\n",
    "\\]\n",
    "\n",
    "#### c) Naucz drzewo regresyjne \\( h_t(x) \\), dopasowujce si do \\( (x_i, r_i^{(t)}) \\)\n",
    "\n",
    "#### d) Zaktualizuj predykcj\n",
    "\n",
    "\\[\n",
    "\\hat{y}_t(x) = \\hat{y}_{t-1}(x) + \\eta \\cdot h_t(x)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Kocowa predykcja\n",
    "\n",
    "#### a) Prawdopodobiestwo klasy 1:\n",
    "\n",
    "\\[\n",
    "P(y = 1 \\mid x) = \\frac{1}{1 + e^{-\\hat{y}_T(x)}}\n",
    "\\]\n",
    "\n",
    "#### b) Klasa kocowa:\n",
    "\n",
    "\\[\n",
    "\\text{klasa}(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{jeli } P(y=1 \\mid x) \\geq 0{,}5 \\\\\n",
    "0 & \\text{w przeciwnym razie}\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "##  Uwagi\n",
    "\n",
    "- Dla klasyfikacji wieloklasowej u偶ywa si softmax zamiast sigmoid.\n",
    "- W praktyce czsto stosuje si te偶 drug pochodn (Newton step).\n",
    "- Popularne implementacje: `XGBoost`, `LightGBM`, `CatBoost`, `sklearn`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf773d",
   "metadata": {},
   "source": [
    "### Zadanie nr 3 (1 punkt)\n",
    "Zbuduj model Gradient Boosting w Pythonie na danych `make_moons` i oce jego dokadno.\n",
    "\n",
    "### Instrukcje\n",
    "1. Wygeneruj zbi贸r danych `make_moons` (500 pr贸bek, szum 0.2).\n",
    "2. Podziel dane na treningowe i testowe (30% testowe).\n",
    "3. Zbuduj model Gradient Boosting z 100 drzewami i learning rate 0.1.\n",
    "4. Wypisz dokadno na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e57bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a8188a",
   "metadata": {},
   "source": [
    "# Random Search\n",
    "\n",
    "Random Search (ang. losowe przeszukiwanie) to metoda strojenia hiperparametr贸w modelu uczenia maszynowego, polegajca na losowym pr贸bkowaniu kombinacji wartoci hiperparametr贸w z zadanego zakresu, zamiast testowania wszystkich mo偶liwych kombinacji (jak w Grid Search). Dla ka偶dej losowej kombinacji model jest trenowany i oceniany (np. za pomoc walidacji krzy偶owej), a najlepsza kombinacja jest wybierana.\n",
    "\n",
    "### Kluczowe cechy:\n",
    "- **Losowo**: Hiperparametry s losowane z zdefiniowanych rozkad贸w (np. jednorodnego, logarytmicznego).\n",
    "- **Efektywno**: Testuje mniejsz liczb kombinacji ni偶 Grid Search, co przyspiesza proces dla du偶ych przestrzeni hiperparametr贸w.\n",
    "- **Elastyczno**: Pozwala na okrelenie zakres贸w cigych (np. learning rate od 0.001 do 0.1) lub dyskretnych (np. liczba drzew: 50, 100, 200).\n",
    "\n",
    "### Matematyka:\n",
    "Niech $ \\Theta $ to przestrze hiperparametr贸w (np. $\\Theta = \\{ \\text{n\\_estimators}, \\text{learning\\_rate}, \\text{max\\_depth} \\}$). Random Search losuje $ n $ kombinacji $ \\theta_1, \\theta_2, \\dots, \\theta_n \\in \\Theta $ z rozkadu (np. jednorodnego) i ocenia model dla ka偶dej $ \\theta_i $ za pomoc funkcji straty $ L(\\theta_i) $ (np. redni bd walidacji krzy偶owej). Wybiera:\n",
    "$$\n",
    "\\theta^* = \\arg\\min_{\\theta_i} L(\\theta_i).\n",
    "$$\n",
    "\n",
    "### Dlaczego Random Search jest wa偶ny?\n",
    "- **Szybsze ni偶 Grid Search**: W du偶ych przestrzeniach hiperparametr贸w testuje mniej kombinacji, zachowujc wysok szans na znalezienie dobrych wartoci.\n",
    "- **Skuteczno w wysokich wymiarach**: Losowe pr贸bkowanie lepiej eksploruje przestrze ni偶 systematyczne grid search, zwaszcza gdy tylko niekt贸re hiperparametry maj du偶e znaczenie.\n",
    "- **Praktyczno**: Umo偶liwia strojenie modeli w rozsdnym czasie, co jest kluczowe w rzeczywistych zastosowaniach, gdzie zasoby obliczeniowe s ograniczone.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703249be",
   "metadata": {},
   "source": [
    "### Zadanie nr 4 (1 punkt)\n",
    "U偶yj Random Search do strojenia hiperparametr贸w modelu XGBoost na danych `make_moons`, testujc wiksz liczb hiperparametr贸w, i wypisz najlepsz dokadno oraz wybrane hiperparametry.\n",
    "\n",
    "### Instrukcje\n",
    "1. Wygeneruj zbi贸r danych `make_moons` (500 pr贸bek, szum 0.2).\n",
    "2. Podziel dane na treningowe i testowe (30% testowe).\n",
    "3. U偶yj `RandomizedSearchCV`, aby znale藕 najlepsze hiperparametry dla XGBoost, testujc: `n_estimators`, `learning_rate`, `max_depth`, `min_child_weight`, `subsample`.\n",
    "4. Wypisz najlepsz dokadno i najlepsze hiperparametry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24133f06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
