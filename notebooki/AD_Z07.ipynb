{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcdad806",
   "metadata": {},
   "source": [
    "# Regresja logistyczna i SVM\n",
    "\n",
    "Opis regresji logistycznej autorstwa Magdaleny Trędowicz został zapożyczony z zajęć Olimpijskiego Koła Sztucznej Inteligencji na Uniwersytecie Jagiellońskim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88350fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93024506",
   "metadata": {},
   "source": [
    "## Regresja logistyczna\n",
    "Załóżmy, że każdy przykład ze zbioru danych $x_i \\in X$ ma przypisaną etykietę $y_i \\in \\{1, \\ldots, K \\}$.\n",
    "Regresja logistyczna jest jednym z klasycznych modeli, który bezpośrednio nadaje się zarówno do klasyfikacji binarnej (dwie klasy: $y_i \\in \\{0,1\\}$), jak i wieloklasowej. \n",
    "Ten model jest o tyle ważny, że stanowi podstawę modeli klasyfikacyjnych opartych o sieci neuronowe. \n",
    "\n",
    "Regresja logistyczna tworzy model probabilistyczny określający prawdopodobieństwo przynależności punktu do poszczególnych klas. \n",
    "W tym modelu chcemy dla każdego punktu wyznaczyć tak zwany rozkład a posteriori $p(1|x_i),\\ldots,p(K|x_i)$ określający przynależność punktu $x_i$ do każdej z $K$ klas. Jako finalną decyzję o klasyfikacji przyjmujemy tę najbardziej prawdopodobną, czyli:\n",
    "\n",
    "$$\n",
    "c(x) = \\arg \\max_{k \\in \\{1, \\dots, K\\}} p(k | x_i).\n",
    "$$\n",
    "\n",
    "Żeby zbudować taki model, musimy sparametryzować prawdopodobieństwa a posteriori, a następnie zbudować funkcję kosztu definiującą kryterium optymalizacyjne. W celu sparametryzowania $p(k|\\cdot)$, określmy moc przyporządkowania punktu $x$ do klasy $k$, wykorzystując model liniowy postaci:\n",
    "\n",
    "$$\n",
    "f_k(x) = w_k^Tx+b_k \\in \\mathbb{R}, \\quad \\text{dla} \\quad k \\in \\{1, \\ldots, K\\}.\n",
    "$$\n",
    "\n",
    "Mając $K$ takich modeli liniowych, możemy wskazać najbardziej prawdopodobną klasę poprzez wyznaczenie największej wartości $f_k(x)$ dla $k \\in \\{1, \\ldots, K\\}$.\n",
    "W celu transformacji tcyh funkcji do prawdopodobieństw wykorzystamy funkcję **softmax** postaci:\n",
    "\n",
    "$$\n",
    "p(k|x) = \\frac{\\exp(z_k)}{\\sum_{j=1}^{K}\\exp(z_j)} \\in (0,1),\n",
    "$$\n",
    "\n",
    "gdzie $z_k = f_k(x) = w_k^Tx + b_k.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2796c",
   "metadata": {},
   "source": [
    "### Zadanie 1 (4 punkty)\n",
    "Wyucz model regresji logistycznej na zbiorze danych Iris, tak że:\n",
    "- Klasyfikator używa tylko jednej zmiennej - szerokości płatka (petal width) oraz wykrywa tylko jedną klasę - Virginica.\n",
    "- Podziel dane na zbiór treningowy, walidacyjny i testowy. Następnie zrób grid search w celu dobrania optymalnego hiperparametru $C$ (jego opis można znaleźć tutaj: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Grid search powinien uwzględnić przynajmniej 10 różnych wartości $C$.\n",
    "- Narysuj wykres przedstawiący dokładność klasyfikatora na zbiorze testowym w zależności od wartości hiperparametru $C$. \n",
    "- Narysuj wykres przedstawiający granicę decyzyjną klasyfikatora. Na osi OX powinna być długość płatka (petal width), a na osi OY prawdopodobieństwo przynależności kwiatu do danej klasy. Taki wykres powinien być narysowany w oparciu o dane testowe.\n",
    "\n",
    "Nie zapomnij podpisać wykresów oraz dodać legendy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a171bb52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'target',\n",
       " 'frame',\n",
       " 'target_names',\n",
       " 'DESCR',\n",
       " 'feature_names',\n",
       " 'filename',\n",
       " 'data_module']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da7c4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b42d3",
   "metadata": {},
   "source": [
    "### Zadanie nr 2 (1 punkt)\n",
    "Powtórz zadanie nr 1, ale tym razem weź pod uwagę dwie zmienne - szerokość i długość płatka (petal width, petal length). Zwizualizuj granicę decyzyjną klasyfikatora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0186318b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6faf65c7",
   "metadata": {},
   "source": [
    "## Maszyny Wektorów Nośnych (SVM)\n",
    "\n",
    "Maszyny Wektorów Nośnych (SVM, ang. Support Vector Machines) to modele nadzorowanego uczenia maszynowego, używane do klasyfikacji i regresji. Główna idea polega na znalezieniu **hiperpłaszczyzny**, która oddziela dane różnych klas z **maksymalnym marginesem**.\n",
    "\n",
    "### Liniowe SVM dla klasyfikacji binarnej\n",
    "\n",
    "Dla zbioru treningowego $ \\{ (\\mathbf{x}_i, y_i) \\}_{i=1}^n $, gdzie $ \\mathbf{x}_i \\in \\mathbb{R}^d $ oraz $ y_i \\in \\{-1, 1\\} $, celem jest znalezienie hiperpłaszczyzny:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b = 0\n",
    "$$\n",
    "\n",
    "która spełnia warunek:\n",
    "\n",
    "$$\n",
    "y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i\n",
    "$$\n",
    "\n",
    "Problem optymalizacyjny ma postać:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b} \\ \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\quad \\text{przy założeniu} \\quad y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1\n",
    "$$\n",
    "\n",
    "Jest to problem wypukłej optymalizacji kwadratowej. Maksymalizujemy margines $ \\frac{2}{\\|\\mathbf{w}\\|} $ między najbliższymi punktami klas, czyli tzw. **wektorami nośnymi**.\n",
    "\n",
    "### Miękki margines (Soft Margin SVM)\n",
    "\n",
    "Aby poradzić sobie z danymi, które nie są liniowo separowalne, wprowadza się tzw. slack variables $ \\xi_i \\geq 0 $:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}} \\ \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i \\quad \\text{przy założeniu} \\quad y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1 - \\xi_i\n",
    "$$\n",
    "\n",
    "gdzie $ C > 0 $ to parametr regularyzacji, który równoważy kompromis między szerokością marginesu a karą za błędy klasyfikacji.\n",
    "\n",
    "### Postać dualna\n",
    "\n",
    "SVM często rozwiązuje się w postaci dualnej. Problem dualny ma postać:\n",
    "\n",
    "$$\n",
    "\\max_{\\boldsymbol{\\alpha}} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^n \\alpha_i \\alpha_j y_i y_j \\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle\n",
    "\\quad \\text{przy założeniach} \\quad\n",
    "0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "Tylko te obserwacje, dla których $ \\alpha_i > 0 $, stają się **wektorami nośnymi**.\n",
    "\n",
    "---\n",
    "\n",
    "## Sztuczka Jądra (Kernel Trick)\n",
    "\n",
    "Gdy dane nie są liniowo separowalne w przestrzeni wejściowej $ \\mathbb{R}^d $, można je przekształcić do przestrzeni cech o wyższej wymiarowości przy pomocy odwzorowania $ \\phi: \\mathbb{R}^d \\rightarrow \\mathcal{H} $. Jednak obliczanie $ \\phi(\\mathbf{x}) $ może być kosztowne lub niewykonalne.\n",
    "\n",
    "**Sztuczka jądra** pozwala tego uniknąć, definiując funkcję jądra $ K $ taką, że:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\langle \\phi(\\mathbf{x}_i), \\phi(\\mathbf{x}_j) \\rangle\n",
    "$$\n",
    "\n",
    "Dzięki temu postać dualna może być wyrażona tylko przy pomocy funkcji jądra:\n",
    "\n",
    "$$\n",
    "\\max_{\\boldsymbol{\\alpha}} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^n \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "### Przykładowe funkcje jądra\n",
    "\n",
    "1. **Jądro liniowe**:\n",
    "$$\n",
    "K(\\mathbf{x}, \\mathbf{z}) = \\mathbf{x}^\\top \\mathbf{z}\n",
    "$$\n",
    "\n",
    "2. **Jądro wielomianowe**:\n",
    "$$\n",
    "K(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^\\top \\mathbf{z} + c)^d\n",
    "$$\n",
    "\n",
    "3. **Jądro RBF / Gaussowskie**:\n",
    "$$\n",
    "K(\\mathbf{x}, \\mathbf{z}) = \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{z}\\|^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "4. **Jądro sigmoidalne** (związane z sieciami neuronowymi):\n",
    "$$\n",
    "K(\\mathbf{x}, \\mathbf{z}) = \\tanh(\\kappa \\mathbf{x}^\\top \\mathbf{z} + \\theta)\n",
    "$$\n",
    "\n",
    "### Funkcja decyzyjna\n",
    "\n",
    "Po nauczeniu modelu, funkcja decyzyjna dla nowego punktu $ \\mathbf{x} $ to:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\text{sign} \\left( \\sum_{i=1}^n \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b \\right)\n",
    "$$\n",
    "\n",
    "Tylko wektory nośne mają wpływ na wartość tej funkcji, co sprawia, że predykcja w SVM jest wydajna obliczeniowo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b1ae9e",
   "metadata": {},
   "source": [
    "### Zadanie 3 (5 punktów)\n",
    "\n",
    "Zbadaj, jak różne jądra (kernel functions) i ich parametry wpływają na jakość klasyfikacji oraz kształt granicy decyzyjnej w klasyfikacji danych make_moons:\n",
    "- Podziel dane na treningowe, testowe i walidacyjne.\n",
    "- Zrób grid searcha po hiperparameterach, by znaleźć te najlepsze. Grid search powinien być zrobiony dla jądra: liniowego, wielomianowego, RBF i sigmoidalnego.\n",
    "- Wykorzstaj metryki: F1 score, precision oraz accuracy do oceny jakości modelu.\n",
    "- Dla każdego jądra z najlepszymi hiperparametrami zwizualizuj granicę decyzyjną.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a95293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dane do zadania\n",
    "moons_data = make_moons(n_samples=300, noise=0.25, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CL_dreams",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
